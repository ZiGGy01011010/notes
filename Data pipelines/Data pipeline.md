https://dataengineering.wiki/Concepts/Data+Pipeline
A Data Pipeline is a term used to describe a workflow consisting of one or more tasks that ingest, move, and transform raw data from one or more sources to a destination.

## ETL
ETL is an acronym for extract, transform, and load. An ETL pipeline generally works by extracting data from a source using custom code or a replication tool, transforming the data using custom code or a transformation tool, and then loading the transformed data to your destination.

- [ETL Pipeline example using Python, Docker, and Airflow](https://github.com/sidharth1805/Spotify_etl)
- [ETL Pipeline example using Airflow, Python/Pandas, and S3](https://github.com/andrem8/surf_dash)
